---
layout: about
title: about
permalink: /
subtitle:

profile:
  align: right
  image: prof_pic.png
  image_circular: false # crops the image to make it circular
  address: ''

news: true  # includes a list of news items
selected_papers: false # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
---

I'm interested in AI safety research. In short, this is because I believe economic and military incentives will continue to push us towards a world filled with increasingly advanced AI systems operating and interacting in ways we--as of now--do not understand.

Within AI safety, I'm most excited about developing [model organisms of misalignment](https://www.alignmentforum.org/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1). While I intuitively believe advanced AI systems may pose existential risks, I find the empirical evidence for such claims underwhelming. Safely experimenting with various threat models would shed light on the realism of these risks, allowing us to better prioritize research directions, convince skeptics, and regulate as necessary.

You can find links to my relevant pages in the icons above and below. I'm always happy to connect.
