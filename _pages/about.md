---
layout: about
title: about
permalink: /
subtitle:

profile:
  align: right
  image: prof_pic.png
  image_circular: false # crops the image to make it circular
  address: ''

news: true  # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
selected_repositories: true # includes links to repositories in 
social: true  # includes social icons at the bottom of the page
---

I'm interested in AI safety research. In short, this is because I believe economic and military incentives will continue to push us towards a world filled with increasingly advanced AI systems operating and interacting in ways we -- as of now -- do not sufficiently understand.

Within AI safety, I'm particularly excited about projects focused on clarifying our understanding of the risks from advanced AI systems, such as developing [model organisms of misalignment](https://www.alignmentforum.org/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1), building [dangerous](https://www.gov.uk/government/publications/emerging-processes-for-frontier-ai-safety/emerging-processes-for-frontier-ai-safety#model-evaluations-and-red-teaming) [capabilities](https://evals.alignment.org) [evaluations](https://openai.com/blog/frontier-risk-and-preparedness), and [red teaming](https://www.anthropic.com/index/frontier-threats-red-teaming-for-ai-safety). While I intuitively believe advanced AI systems may pose serious risks, I find existing empirical evidence for such claims underwhelming. Safely experimenting with various threat models would shed light on the realism of these risks, allowing us to better prioritize research directions, convince skeptics, and regulate as necessary.

You can find links to my relevant pages in the icons above and below. I'm always happy to connect.
